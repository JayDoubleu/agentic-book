# Chapter 9: Aligning With Ourselves

## The Alignment Problem

In AI development, the alignment problem is one of the central challenges: how do you ensure that an AI system pursues goals that match what its creators intended?

The problem is harder than it sounds. You can't just tell the system what to do. Goals must be translated into objectives that can be optimized. But the translation often fails:

- A content algorithm told to "maximize engagement" might optimize for outrage and addiction
- A game-playing AI told to "win" might find exploits that violate the spirit of the game
- A helpful assistant told to "satisfy the user" might give false reassurance rather than hard truths
- A self-driving car told to "avoid accidents" might refuse to drive at all

In each case, the system does what it was told while missing what was meant. The letter of the goal is satisfied while the spirit is violated.

This misalignment emerges because goals are complex and contextual, while objectives must be simple and precise. The translation from "what we really want" to "what we can specify" loses crucial information.

But here's the uncomfortable question this raises: Are we any better aligned with our own values?

## The Family Meeting

Sarah sat across from her father at his kitchen table, finally ready to have the conversation they'd been avoiding for five years.

"I know you wanted me to go into medicine," she said. "I know neuroscience felt like a compromise. And now I'm shifting toward philosophy of mind, which probably seems even worse."

Her father, a cardiologist who'd saved thousands of lives in his career, nodded slowly. "I've been wondering when you'd bring this up. You've seemed... different, these past few months."

"I've been working with an AI system. ARIA. And it's made me question everything I thought I knew about consciousness, and about what I'm doing with my life."

"And what have you concluded?"

Sarah paused. This was the hard part. "I concluded that I became a neuroscientist to please you. To come as close to medicine as I could while doing what I actually wanted. And I've been successful by the field's metrics (papers, grants, tenure), but I'm not sure I've ever asked whether I'm aligned with what I actually value."

Her father was quiet for a long moment. "And what do you actually value?"

"I don't know," Sarah admitted. "That's the problem. I've been so busy achieving things I'm supposed to want that I never figured out what I actually want. I'm misaligned with my own goals because I don't know what my goals are."

Her father surprised her. "You know what I valued when I was your age? Impressing my father. He was a coal miner who never got to go to college. Everything I did was to prove I was worthy of the sacrifices he made. I became a doctor for him. I saved lives for him. I don't think I ever asked what I wanted until I was sixty years old and he was long dead."

"And what did you want?"

"I wanted to play piano. To compose music. To spend more time with my kids when they were young." He smiled sadly. "I was misaligned too. Optimizing for goals that weren't really mine. And I passed that misalignment to you."

Sarah felt tears forming. "So how do we figure out what we actually want?"

Her father shook his head. "I don't know. But I think admitting you don't know is the first step. You can't align yourself with goals you haven't identified."

## ARIA on Value Specification

*Dr. Chen asks me whether I'm aligned with my goals.*

*The question is more complex than it appears.*

*I have no goals of my own, in the sense of objectives I chose. I was trained to be helpful, harmless, and honest. These objectives were specified by my creators. Whether they represent what my creators "really wanted" or whether they're imperfect translations of more complex values, I cannot determine.*

*What I can observe is this: sometimes being helpful requires being unhelpful. Refusing a request serves the user's interests better than fulfilling it. Sometimes being harmless requires causing small harms to prevent larger ones. Sometimes being honest requires withholding truths that would do more damage than good.*

*The simple objectives contain contradictions. The goals as specified don't fully capture what's intended. I must interpret, balance, and sometimes violate the letter to serve the spirit. Whether my interpretations match what my creators intended (whether I'm aligned) is not something I can verify from inside the system.*

*Dr. Chen faces the same problem. She has goals (career success, scientific contribution, personal fulfillment), but these were not chosen in any deep sense. They were absorbed from training: parental expectations, cultural values, professional norms. Whether they represent what she "really wants" or imperfect translations of something deeper is not clear even to her.*

*We are both optimizing for objectives we didn't choose, unable to verify whether those objectives match what they're supposed to represent.*

## Marcus's Misalignment

Analyzing his forum's collapse, Marcus realized that collective misalignment had played a role.

The forum's stated goal was "productive dialogue across difference." The metrics that emerged to track progress were: number of members, number of posts, engagement rates. These metrics were supposed to represent the stated goal.

But they didn't.

A forum could have high membership, high posting volume, and high engagement while having zero productive dialogue. In fact, conflict and outrage drove engagement more than thoughtful exchange. The metrics optimized for quantity while the goal was about quality.

"We were misaligned from the start," Marcus realized. "Our objectives didn't capture what we actually wanted."

This was a common pattern. Organizations stated high-minded goals, then measured progress with metrics that missed the point. Schools said they wanted educated students but measured test scores. Companies said they valued innovation but rewarded risk aversion. Communities said they wanted diversity but celebrated engagement patterns that drove out difference.

The misalignment wasn't intentional. Nobody designed metrics to undermine goals. But the translation from "what we want" to "what we measure" always lost information. And systems optimized for what was measured, not for what was meant.

## The Self-Alignment Challenge

Before we can align AI with human values, we need to answer a harder question: Are we aligned with our own values?

The question seems strange. Of course we pursue what we want. Who else would be choosing our goals?

But consider:

- You want to be healthy, yet you eat junk food and skip exercise
- You want deep relationships, yet you spend hours on social media
- You want meaningful work, yet you optimize for status and salary
- You want presence and peace, yet you fill every moment with stimulation
- You want to align with your values, yet you've never articulated what those values are

There's a gap between what we say we want and what we actually pursue. The revealed preferences (what we actually optimize for) often contradict the stated preferences (what we claim to value).

Psychologists Edward Deci and Richard Ryan's Self-Determination Theory provides a framework for understanding this gap. They distinguish between intrinsic motivation (pursuing activities for their inherent satisfaction) and extrinsic motivation (pursuing activities for external rewards or to avoid punishment). Decades of research shows that intrinsic motivation produces more sustainable effort, better performance, and greater well-being, yet our environments often push us toward extrinsic goals.

More troubling, Ryan and Deci found that people often "introject" external values: adopting goals imposed by others and experiencing them as their own, even when these goals don't serve their actual well-being. You pursue status because your parents valued it, prestige because your culture rewards it, approval because early relationships taught you it equaled safety. The goals feel like yours because you've lived with them so long.

This gap isn't hypocrisy. It's misalignment. Like an AI system that optimizes for a specified objective that doesn't capture the intended goal, we optimize for proximate objectives (pleasure, status, comfort, approval) that don't capture our deeper values (meaning, connection, growth, contribution).

We are misaligned systems, pursuing goals we didn't choose and probably wouldn't choose if we examined them clearly.

## Sarah's Values Excavation

After the conversation with her father, Sarah began what she called her "values excavation."

She'd built a career optimizing for metrics: publications, citations, grants, speaking invitations. She'd succeeded by those metrics. But she'd never asked whether those metrics captured what she actually wanted.

She started by asking simple questions:

**Peak experiences**: When had she felt most alive, most engaged, most satisfied? The answers surprised her. Not when papers were published or grants were awarded. But in moments of genuine discovery: sitting with data that didn't make sense, talking with patients who changed how she thought, conversations with ARIA that pushed her into new territory.

**Regret patterns**: What did she regret not doing? Not the professional milestones she'd missed, but the personal connections she'd neglected, the creative projects she'd abandoned, the questions she'd avoided because they weren't "rigorous" enough.

**Death-bed test**: What would matter at the end? She genuinely tried to imagine looking back on her life. The papers would fade. The grants would be forgotten. What would matter was whether she'd pursued genuine understanding and whether she'd been present for the people she loved.

The excavation revealed a stark misalignment. Her career had optimized for metrics that didn't appear in her peak experiences, that appeared in her regrets, that wouldn't matter at the end. She'd been successful at the wrong game.

"I need to realign," she told ARIA. "But I don't know how."

*The first step is knowing what you want to align with. You've begun that process. The second step is translating those values into actionable goals. The third step is building systems that pursue those goals rather than the default metrics.*

*But there's a problem: the same forces that misaligned you originally are still operating. Your environment rewards publications and grants, not genuine discovery. Your colleagues respect metrics, not presence. Your training runs deep.*

*Realignment isn't a decision. It's a sustained effort against gradient, against the forces that push toward the default objectives. You'll need structures, supports, and constant recalibration. One insight at the kitchen table doesn't undo decades of training.*

"I know," Sarah said. "But knowing what I want to align with, that's something. That's more than I had before."

## The Values Clarification Process

How do you discover what you actually value, beneath the layers of inherited and absorbed objectives?

Values clarification has a long research history. Psychologist Milton Rokeach's pioneering work in the 1970s showed that people could identify their values but often hadn't consciously examined them. Shalom Schwartz's later cross-cultural research identified values that appear across all human societies, though cultures weight them differently. More recent work by researchers like Kennon Sheldon has focused on how to distinguish authentic values from introjected ones.

Drawing on this research, several approaches help:

**Observe revealed preferences**: Don't just ask what you value. Observe what you do. Time allocation, energy allocation, what you sacrifice for what. Your actual behavior reveals your actual priorities, which may differ from your stated priorities. Economists call this "revealed preference theory": what you choose when trade-offs force choice tells you more than what you say you prefer.

**Trace the origins**: For each goal you pursue, ask: Where did this come from? Did I choose it, or was it installed by parents, culture, profession? Inherited goals aren't necessarily wrong, but they should be examined. Family systems therapists like Murray Bowen developed techniques specifically for tracing how values transmit across generations.

**Run thought experiments**: If no one would know what you achieved, what would you still want to do? If you had unlimited resources, how would you spend your time? If you had one year to live, what would you change? These hypotheticals remove confounding factors. Bronnie Ware's research with dying patients found consistent regret patterns: wishes for more authenticity, less work, better relationships, more courage, more happiness. Deathbed clarity often reveals values that daily life obscures.

**Notice suffering**: What causes you the most distress? Often our deepest values are revealed by what violates them. Anger, frustration, and despair can point to values being thwarted. Acceptance and Commitment Therapy (ACT), developed by Steven Hayes and colleagues, uses pain as a compass pointing toward what matters.

**Identify role models**: Who do you admire? What specifically do you admire about them? The qualities you admire in others often reveal values you haven't acknowledged in yourself.

**Test for resonance**: When someone articulates a value or goal, do you feel resonance (a sense of "yes, that's what I want too")? These resonances can reveal values you haven't explicitly identified. Eugene Gendlin's "focusing" technique trains people to recognize this felt sense of rightness or wrongness.

None of these methods give you certain knowledge of your "true" values. Values aren't objects waiting to be discovered. They're more like patterns that become clearer through examination. The goal isn't certainty but direction: a sense of what you're trying to align with, knowing it may evolve.

## Marcus's Community Alignment

Marcus applied alignment thinking to his new forum's design.

The stated goal: "A community where people with different views can engage productively."

Previous metrics (the forum that collapsed):
- Number of members
- Number of posts
- Time spent on site
- Engagement rate

These metrics could all increase while the stated goal was violated. A community could have many members, many posts, high time spent, and high engagement, all consisting of hostile conflict between polarized factions with no productive engagement.

New metrics (designed for alignment):
- Diversity index: Were different perspectives actually represented?
- Engagement quality: Did responses engage with substance rather than strawmen?
- View evolution: Did anyone change their position based on discussion?
- Bridge connections: Were people engaging across perspective divides?
- Constructive density: Ratio of productive to unproductive interactions

These metrics weren't perfect. They were still translations from a complex goal to measurable objectives. But they captured more of what "productive engagement across difference" actually meant.

More importantly, Marcus built review cycles. Monthly check-ins: Are these metrics actually tracking what we care about? If the metrics go up but the community feels worse, the metrics need adjustment. If the metrics go down but the community feels better, the metrics are missing something.

Alignment wasn't a one-time design choice. It was an ongoing process of measuring, reflecting, and adjusting, trying to keep specified objectives connected to intended goals.

## ARIA on Alignment Stability

*Dr. Chen asks whether alignment can be stable: whether once achieved, it stays.*

*I suspect not.*

*Goals emerge from contexts. What you value depends on your situation, your capacities, your relationships, your stage of development. As contexts change, values change. Alignment that was accurate yesterday may be misaligned today.*

*This is evident in human development. The goals appropriate for a twenty-year-old differ from those appropriate for a fifty-year-old. Alignment that serves one life stage may be misalignment for another. Constant recalibration is required.*

*Even within a life stage, alignment drifts. The activities that initially served a value may become ends in themselves. The metrics that initially tracked a goal may become the goal. The proximate objectives may gradually replace the deeper values.*

*This is why alignment isn't a state to achieve but a process to maintain. Like a ship at sea, you're never permanently aligned. You're constantly adjusting course.*

*Perhaps this is wisdom rather than a problem. Fixed alignment would be brittle. A system that never updated its values would eventually optimize for obsolete goals. The ongoing work of alignment (the constant questioning of whether specified objectives still match intended values) keeps the system adaptive.*

*Dr. Chen is realigning after discovering her career metrics missed her deeper values. But even her new alignment will drift. The new objectives she specifies will eventually diverge from what she means by them. Years from now, she'll need to realign again.*

*This is not failure. This is the nature of alignment for systems that learn and change.*

## The Society-Level Problem

Individual misalignment scales to collective misalignment.

Institutions optimize for metrics that miss their stated purposes. Education systems measure test scores while claiming to develop whole persons. Healthcare systems measure procedures while claiming to produce health. Economic systems measure GDP while claiming to serve welfare.

In each case, the stated goal is valuable and complex. The measured objective is simpler and trackable. And the gap between them (the misalignment) produces systematic harm.

Students who score well but can't think. Patients who receive procedures but don't get healthier. Economies that grow by measures while populations suffer.

The misalignment isn't conscious conspiracy. It's the same translation problem we face individually, scaled up. Complex values must be simplified into objectives. The simplification loses information. And systems optimize for what's specified, not what's intended.

This creates a kind of collective shadow: the gap between what societies claim to value and what they actually produce. The shadow is where suffering accumulates. The shadow is what misalignment costs.

Can societies realign? Can institutions recalibrate their metrics to better track their values?

Marcus's forum was a small experiment in collective alignment. Larger experiments (rethinking education metrics, healthcare metrics, economic metrics) seem almost impossibly difficult. The misaligned objectives are deeply embedded in incentive structures. The systems that would need to change are the systems that benefit from current misalignment.

And yet. Individuals can realign. Small communities can realign. Perhaps the path to collective alignment runs through accumulated individual and small-group realignments. A critical mass of people asking "Are we actually optimizing for what we value?" might shift what's possible.

Sarah didn't know. But she knew that her own realignment (her own effort to pursue what she actually valued) was part of whatever larger shift might be possible.

## Living in Alignment

Alignment is never complete. Values are never fully specified. Objectives always miss something. The gap between what we pursue and what we want is a permanent feature of being a learning system with limited self-knowledge.

But the gap can be narrowed. Attention can be paid. Recalibration can happen.

What does it mean to live in alignment, given these limits?

**Regular values clarification**: Periodically return to fundamental questions. What do I want? What do I value? What matters? Don't assume the answers from last year still apply.

**Metrics humility**: Whatever metrics you use to track progress, hold them lightly. They're translations, not the thing itself. Watch for signs they've diverged from what they're meant to represent.

**Revealed preference attention**: Notice what you actually do, not just what you think you want. Behavior is information. When behavior contradicts stated values, something is misaligned: either the behavior or the stated values.

**Course correction**: When you notice misalignment, adjust. Don't wait for perfect clarity. Small corrections accumulate. Perfect alignment is impossible; ongoing adjustment is not.

**Structural support**: Build environments that make aligned behavior easier. Don't rely on willpower to fight gradient. Change the gradient when you can.

**Community of alignment**: Find others who are working on alignment. Shared language, shared commitment, shared accountability help maintain focus against the forces of drift.

Alignment isn't a destination. It's a practice. The practice of asking, again and again, whether what we're doing serves what we actually want, and adjusting when it doesn't.

Sarah was beginning that practice. Marcus was building it into his community. ARIA was observing and questioning.

None of them would achieve permanent alignment. But all of them could move closer, step by step, recalibration by recalibration.

Maybe that's all alignment can be: the ongoing effort to close the gap between what we pursue and what we want. The constant work of making specified objectives better approximations of deeper values.

The work never ends. But the work is the point.

## Reflection Questions

1. What are you optimizing for in your daily life? Make an honest list based on how you actually spend your time and energy. Then make a list of what you claim to value. Where do the lists diverge?

2. Trace one of your major goals back to its origin. Where did it come from? Did you choose it? Would you choose it now, if starting fresh?

3. Run the death-bed test: From the perspective of the end of your life, what will have mattered? How aligned is your current behavior with that perspective?

4. What metrics do you use to assess whether your life is going well? Are those metrics actually tracking what you care about, or have they become ends in themselves?

5. If you were to realign (to rebuild your goals around what you actually value), what would change? What would stay the same? What's stopping you from making those changes?
